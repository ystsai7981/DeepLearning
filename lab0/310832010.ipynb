{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43313c5a-eaac-4625-99ab-fe476cfded0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "65722e19-652e-4cf8-81b5-deb61e094708",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "device = 'cuda'\n",
    "epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "197fd4d9-d2fd-4f77-8ddb-ae5220e34798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設置seed\n",
    "seed = 198964\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0c35a5a6-0f30-4755-80a0-a557eac12f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "train_data = datasets.MNIST('./MNIST_data', train=True, download=True,\n",
    "                            transform=transforms.Compose([\n",
    "                                transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.1307,), (0.3081,))\n",
    "                            ]))\n",
    "test_data = datasets.MNIST('./MNIST_data', train=False, download=True,\n",
    "                           transform=transforms.Compose([\n",
    "                               transforms.ToTensor(),\n",
    "                               transforms.Normalize((0.1307,),(0.3081,))\n",
    "                           ]))\n",
    "val_data, test_data = torch.utils.data.random_split(dataset=test_data, lengths=[5000,5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a35ba60a-a5df-4dde-97c4-638a68c43758",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataloader\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True, num_workers=3)\n",
    "val_loader = torch.utils.data.DataLoader(val_data, batch_size=1000, shuffle=True, num_workers=3)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1000, shuffle=True, num_workers=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e2749d",
   "metadata": {},
   "source": [
    "增加兩行convolution layers，並增加neuron數量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23169ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# module\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=(5,5),stride=1, padding=0)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=(5,5),stride=1, padding=0)\n",
    "        self.fc1 = nn.Linear(16*4*4, 120) # make neuron 2x wider\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.conv1(x))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = F.relu(self.conv2(out))\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = torch.flatten(out, start_dim=1) #flatten\n",
    "        out = F.relu(self.fc1(out))\n",
    "        out = F.relu(self.fc2(out))\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "model = Net().to(device) # build model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dac71fb",
   "metadata": {},
   "source": [
    "建model並設置優化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "537e2d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define optimizer/loss function\n",
    "Loss = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "# optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "# optimizer = optim.RMSprop(model.parameters(), lr=0.1, momentum=0, alpha=0.5) # Learning rate here will be replaced by next function. By the way, alpha is set to 0.99 defaultly, and I set it to 0.5 . I don't want any momentum as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff77f3",
   "metadata": {},
   "source": [
    "設定learning rate的排程，使他在多個epoch之後逐漸降低"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da9a4fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RMSprop learning rate scheduling\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    if epoch < 5: # After 5 epochs, everage loss always decrease slowly, so we can use smaller learning rate to avoid too much change\n",
    "        lr = 0.005  # 0.01 is too big so let's try 0.005\n",
    "    elif epoch < 15:\n",
    "        lr = 0.0005 \n",
    "    else: \n",
    "        lr = 0.00005\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "98632db7-6f46-4460-bf53-405f734a5f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusting learning rate of group 0 to 1.0000e-02.\n"
     ]
    }
   ],
   "source": [
    "lr_scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer=optimizer, T_max=epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "35ff4a38-3e9f-4ca0-90c8-58cfe960b4ca",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#training function\n",
    "def train(model, loss_func, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = loss_func(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2ff9b607-6de5-4418-8b8b-8bdc9f23b8d8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [44928/60000 (100%)]\tLoss: 0.581821\n",
      "\n",
      "Test set: Average loss: 0.5045, Accuracy: 4222/5000 (84%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 9.9726e-03.\n",
      "Train Epoch: 2 [44928/60000 (100%)]\tLoss: 0.312030\n",
      "\n",
      "Test set: Average loss: 0.2866, Accuracy: 4516/5000 (90%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 9.8907e-03.\n",
      "Train Epoch: 3 [44928/60000 (100%)]\tLoss: 0.215037\n",
      "\n",
      "Test set: Average loss: 0.1941, Accuracy: 4701/5000 (94%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 9.7553e-03.\n",
      "Train Epoch: 4 [44928/60000 (100%)]\tLoss: 0.305573\n",
      "\n",
      "Test set: Average loss: 0.1371, Accuracy: 4788/5000 (96%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 9.5677e-03.\n",
      "Train Epoch: 5 [44928/60000 (100%)]\tLoss: 0.091997\n",
      "\n",
      "Test set: Average loss: 0.1064, Accuracy: 4835/5000 (97%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 9.3301e-03.\n",
      "Train Epoch: 6 [44928/60000 (100%)]\tLoss: 0.109248\n",
      "\n",
      "Test set: Average loss: 0.0937, Accuracy: 4863/5000 (97%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 9.0451e-03.\n",
      "Train Epoch: 7 [44928/60000 (100%)]\tLoss: 0.093473\n",
      "\n",
      "Test set: Average loss: 0.0840, Accuracy: 4871/5000 (97%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 8.7157e-03.\n",
      "Train Epoch: 8 [44928/60000 (100%)]\tLoss: 0.095514\n",
      "\n",
      "Test set: Average loss: 0.0764, Accuracy: 4876/5000 (98%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 8.3457e-03.\n",
      "Train Epoch: 9 [44928/60000 (100%)]\tLoss: 0.056202\n",
      "\n",
      "Test set: Average loss: 0.0711, Accuracy: 4898/5000 (98%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 7.9389e-03.\n",
      "Train Epoch: 10 [44928/60000 (100%)]\tLoss: 0.032531\n",
      "\n",
      "Test set: Average loss: 0.0671, Accuracy: 4889/5000 (98%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 7.5000e-03.\n",
      "Train Epoch: 11 [44928/60000 (100%)]\tLoss: 0.130998\n",
      "\n",
      "Test set: Average loss: 0.0641, Accuracy: 4906/5000 (98%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 7.0337e-03.\n",
      "Train Epoch: 12 [44928/60000 (100%)]\tLoss: 0.067656\n",
      "\n",
      "Test set: Average loss: 0.0582, Accuracy: 4909/5000 (98%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 6.5451e-03.\n",
      "Train Epoch: 13 [44928/60000 (100%)]\tLoss: 0.100515\n",
      "\n",
      "Test set: Average loss: 0.0599, Accuracy: 4898/5000 (98%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 6.0396e-03.\n",
      "Train Epoch: 14 [44928/60000 (100%)]\tLoss: 0.038055\n",
      "\n",
      "Test set: Average loss: 0.0546, Accuracy: 4910/5000 (98%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 5.5226e-03.\n",
      "Train Epoch: 15 [44928/60000 (100%)]\tLoss: 0.042226\n",
      "\n",
      "Test set: Average loss: 0.0521, Accuracy: 4923/5000 (98%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 5.0000e-03.\n",
      "Train Epoch: 16 [44928/60000 (100%)]\tLoss: 0.106313\n",
      "\n",
      "Test set: Average loss: 0.0526, Accuracy: 4920/5000 (98%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.4774e-03.\n",
      "Train Epoch: 17 [44928/60000 (100%)]\tLoss: 0.045381\n",
      "\n",
      "Test set: Average loss: 0.0532, Accuracy: 4920/5000 (98%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 3.9604e-03.\n",
      "Train Epoch: 18 [44928/60000 (100%)]\tLoss: 0.063077\n",
      "\n",
      "Test set: Average loss: 0.0492, Accuracy: 4929/5000 (99%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 3.4549e-03.\n",
      "Train Epoch: 19 [44928/60000 (100%)]\tLoss: 0.049799\n",
      "\n",
      "Test set: Average loss: 0.0519, Accuracy: 4920/5000 (98%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 2.9663e-03.\n",
      "Train Epoch: 20 [44928/60000 (100%)]\tLoss: 0.020366\n",
      "\n",
      "Test set: Average loss: 0.0467, Accuracy: 4933/5000 (99%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 2.5000e-03.\n",
      "Train Epoch: 21 [44928/60000 (100%)]\tLoss: 0.043296\n",
      "\n",
      "Test set: Average loss: 0.0472, Accuracy: 4933/5000 (99%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 2.0611e-03.\n",
      "Train Epoch: 22 [44928/60000 (100%)]\tLoss: 0.028724\n",
      "\n",
      "Test set: Average loss: 0.0464, Accuracy: 4931/5000 (99%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 1.6543e-03.\n",
      "Train Epoch: 23 [44928/60000 (100%)]\tLoss: 0.032837\n",
      "\n",
      "Test set: Average loss: 0.0463, Accuracy: 4936/5000 (99%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 1.2843e-03.\n",
      "Train Epoch: 24 [44928/60000 (100%)]\tLoss: 0.034128\n",
      "\n",
      "Test set: Average loss: 0.0462, Accuracy: 4935/5000 (99%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 9.5492e-04.\n",
      "Train Epoch: 25 [44928/60000 (100%)]\tLoss: 0.026265\n",
      "\n",
      "Test set: Average loss: 0.0455, Accuracy: 4936/5000 (99%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 6.6987e-04.\n",
      "Train Epoch: 26 [44928/60000 (100%)]\tLoss: 0.005464\n",
      "\n",
      "Test set: Average loss: 0.0453, Accuracy: 4938/5000 (99%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 4.3227e-04.\n",
      "Train Epoch: 27 [44928/60000 (100%)]\tLoss: 0.038612\n",
      "\n",
      "Test set: Average loss: 0.0453, Accuracy: 4935/5000 (99%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 2.4472e-04.\n",
      "Train Epoch: 28 [44928/60000 (100%)]\tLoss: 0.039439\n",
      "\n",
      "Test set: Average loss: 0.0451, Accuracy: 4936/5000 (99%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 1.0926e-04.\n",
      "Train Epoch: 29 [44928/60000 (100%)]\tLoss: 0.030281\n",
      "\n",
      "Test set: Average loss: 0.0453, Accuracy: 4936/5000 (99%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 2.7391e-05.\n",
      "Train Epoch: 30 [44928/60000 (100%)]\tLoss: 0.044204\n",
      "\n",
      "Test set: Average loss: 0.0452, Accuracy: 4936/5000 (99%)\n",
      "\n",
      "Adjusting learning rate of group 0 to 0.0000e+00.\n"
     ]
    }
   ],
   "source": [
    "#Testing function\n",
    "def test(model, loss_func, optimizer):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for batch_idx, (data, target) in enumerate(val_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(data)\n",
    "            test_loss += loss_func(output, target).item()\n",
    "            pred = output.argmax(dim=1) # get the index of the max log-probability\n",
    "            correct += pred.eq(target.data).sum()\n",
    "\n",
    "    test_loss /= len(test_loader) # loss function already averages over batch size\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "\n",
    "#run and save model\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, Loss, optimizer, epoch)\n",
    "    test(model, Loss, optimizer)\n",
    "    lr_scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ec725-ecfd-4c2e-b874-9270e23883a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
