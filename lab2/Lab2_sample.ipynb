{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EuCYJSSDFukq"
   },
   "source": [
    "# Lab 2 Sample Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "v0BmuW9dcCId"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "v0BmuW9dcCId"
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    \"\"\" Sigmoid function.\n",
    "    This function accepts any shape of np.ndarray object as input and perform sigmoid operation.\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "def der_sigmoid(y):\n",
    "    \"\"\" First derivative of Sigmoid function.\n",
    "    The input to this function should be the value that output from sigmoid function.\n",
    "    \"\"\"\n",
    "    return y * (1 - y)\n",
    "\n",
    "\n",
    "class GenData:\n",
    "    @staticmethod\n",
    "    def _gen_linear(n=100):\n",
    "        \"\"\" Data generation (Linear)\n",
    "\n",
    "        Args:\n",
    "            n (int):    the number of data points generated in total.\n",
    "\n",
    "        Returns:\n",
    "            data (np.ndarray, np.float):    the generated data with shape (n, 2). Each row represents\n",
    "                a data point in 2d space.\n",
    "            labels (np.ndarray, np.int):    the labels that correspond to the data with shape (n, 1).\n",
    "                Each row represents a corresponding label (0 or 1).\n",
    "        \"\"\"\n",
    "        data = np.random.uniform(0, 1, (n, 2))\n",
    "\n",
    "        inputs = []\n",
    "        labels = []\n",
    "\n",
    "        for point in data:\n",
    "            inputs.append([point[0], point[1]])\n",
    "\n",
    "            if point[0] > point[1]:\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(1)\n",
    "\n",
    "        return np.array(inputs), np.array(labels).reshape((-1, 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def _gen_xor(n=100):\n",
    "        \"\"\" Data generation (XOR)\n",
    "\n",
    "        Args:\n",
    "            n (int):    the number of data points generated in total.\n",
    "\n",
    "        Returns:\n",
    "            data (np.ndarray, np.float):    the generated data with shape (n, 2). Each row represents\n",
    "                a data point in 2d space.\n",
    "            labels (np.ndarray, np.int):    the labels that correspond to the data with shape (n, 1).\n",
    "                Each row represents a corresponding label (0 or 1).\n",
    "        \"\"\"\n",
    "        data_x = np.linspace(0, 1, n // 2)\n",
    "\n",
    "        inputs = []\n",
    "        labels = []\n",
    "\n",
    "        for x in data_x:\n",
    "            inputs.append([x, x])\n",
    "            labels.append(0)\n",
    "\n",
    "            if x == 1 - x:\n",
    "                continue\n",
    "\n",
    "            inputs.append([x, 1 - x])\n",
    "            labels.append(1)\n",
    "\n",
    "        return np.array(inputs), np.array(labels).reshape((-1, 1))\n",
    "\n",
    "    @staticmethod\n",
    "    def fetch_data(mode, n):\n",
    "        \"\"\" Data gather interface\n",
    "\n",
    "        Args:\n",
    "            mode (str): 'Linear' or 'XOR', indicate which generator is used.\n",
    "            n (int):    the number of data points generated in total.\n",
    "        \"\"\"\n",
    "        assert mode == 'Linear' or mode == 'XOR'\n",
    "\n",
    "        data_gen_func = {\n",
    "            'Linear': GenData._gen_linear,\n",
    "            'XOR': GenData._gen_xor\n",
    "        }[mode]\n",
    "\n",
    "        return data_gen_func(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "v0BmuW9dcCId"
   },
   "outputs": [],
   "source": [
    "class SimpleNet:\n",
    "    def __init__(self, num_step=2000, print_interval=100):\n",
    "        \"\"\" A hand-crafted implementation of simple network.\n",
    "\n",
    "        Args:\n",
    "            num_step (optional):    the total number of training steps.\n",
    "            print_interval (optional):  the number of steps between each reported number.\n",
    "        \"\"\"\n",
    "        self.num_step = num_step\n",
    "        self.print_interval = print_interval\n",
    "        self.TRAIN = True\n",
    "\n",
    "        # Model parameters initialization\n",
    "        # hidden layer 1: 100 nodes\n",
    "        # hidden layer 2: 10 nodes\n",
    "        # Please initiate your network parameters here.\n",
    "        self.w1 = np.random.uniform(0, 1, (2, 100))\n",
    "        self.w2 = np.random.uniform(0, 1, (100, 10))\n",
    "        self.w3 = np.random.uniform(0, 1, (10, 1))\n",
    "        self.b1 = np.random.uniform(0, 1, (1,))\n",
    "        self.b2 = np.random.uniform(0, 1, (1,))\n",
    "        self.b3 = np.random.uniform(0, 1, (1,))\n",
    "\n",
    "        \n",
    "\n",
    "    @staticmethod\n",
    "    def plot_result(data, gt_y, pred_y):\n",
    "        \"\"\" Data visualization with ground truth and predicted data comparison. There are two plots\n",
    "        for them and each of them use different colors to differentiate the data with different labels.\n",
    "\n",
    "        Args:\n",
    "            data:   the input data\n",
    "            gt_y:   ground truth to the data\n",
    "            pred_y: predicted results to the data\n",
    "        \"\"\"\n",
    "        print(data.shape, gt_y.shape, pred_y.shape[0])\n",
    "        assert data.shape[0] == gt_y.shape[0]\n",
    "        assert data.shape[0] == pred_y.shape[0]\n",
    "\n",
    "        plt.figure()\n",
    "\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title('Ground Truth', fontsize=18)\n",
    "\n",
    "        for idx in range(data.shape[0]):\n",
    "            if gt_y[idx] == 0:\n",
    "                plt.plot(data[idx][0], data[idx][1], 'ro')\n",
    "            else:\n",
    "                plt.plot(data[idx][0], data[idx][1], 'bo')\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title('Prediction', fontsize=18)\n",
    "\n",
    "        for idx in range(data.shape[0]):\n",
    "            if pred_y[idx] == 0:\n",
    "                plt.plot(data[idx][0], data[idx][1], 'ro')\n",
    "            else:\n",
    "                plt.plot(data[idx][0], data[idx][1], 'bo')\n",
    "\n",
    "        plt.show()\n",
    "        \n",
    "    def train(self):\n",
    "        self.TRAIN = True\n",
    "        \n",
    "    def evaluate(self):\n",
    "        self.TRAIN = False\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\" Implementation of the forward pass.\n",
    "        It should accepts the inputs and passing them through the network and return results.\n",
    "        \"\"\"\n",
    "\n",
    "        \"\"\" FILL IN HERE \"\"\"\n",
    "        self.inputs = inputs\n",
    "        self.l1_out = sigmoid(dot(inputs, self.w1) + self.b1)\n",
    "        self.l2_out = sigmoid(dot(self.l1_out, self.w2) + self.b2)\n",
    "        self.l3_out = sigmoid(dot(self.l2_out, self.w3) + self.b3)\n",
    "        return self.l3_out\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\" Implementation of the backward pass.\n",
    "        It should utilize the saved loss to compute gradients and update the network all the way to the front.\n",
    "        \"\"\"\n",
    "        learning_rate = 1\n",
    "        \n",
    "        \"\"\" FILL IN HERE \"\"\"\n",
    "        gradient = self.error * der_sigmoid(self.l3_out)\n",
    "        # tmp = der_sigmoid(dot(gradient, self.w3.transpose()))\n",
    "        self.w3 -= self.l2_out.transpose() * gradient * learning_rate\n",
    "        gradient = der_sigmoid(self.l2_out) * gradient\n",
    "        # tmp = der_sigmoid(dot(gradient, self.w2.transpose()))\n",
    "        self.w2 -= self.l1_out.transpose() * gradient * learning_rate\n",
    "        gradient = der_sigmoid(self.l1_out) * gradient\n",
    "        # tmp = der_sigmoid(dot(gradient, self.w1.transpose()))\n",
    "        self.w1 -= self.inputs.transpose() * gradient * learning_rate\n",
    "            \n",
    "\n",
    "\n",
    "    def train(self, inputs, labels):\n",
    "        \"\"\" The training routine that runs and update the model.\n",
    "\n",
    "        Args:\n",
    "            inputs: the training (and testing) data used in the model.\n",
    "            labels: the ground truth of correspond to input data.\n",
    "        \"\"\"\n",
    "        # make sure that the amount of data and label is match\n",
    "        assert inputs.shape[0] == labels.shape[0]\n",
    "\n",
    "        n = inputs.shape[0]\n",
    "\n",
    "        for epochs in range(self.num_step):\n",
    "            for idx in range(n):\n",
    "                # operation in each training step:\n",
    "                #   1. forward passing\n",
    "                #   2. compute loss\n",
    "                #   3. propagate gradient backward to the front\n",
    "                self.output = self.forward(inputs[idx:idx+1, :])\n",
    "                self.error = self.output - labels[idx:idx+1, :]\n",
    "                \"\"\" apply your backward function: \"\"\"\n",
    "                \"\"\" FILL IN HERE \"\"\"\n",
    "                self.backward()\n",
    "\n",
    "            if epochs % self.print_interval == 0:\n",
    "                print('Epochs {}: '.format(epochs))\n",
    "                self.test(inputs, labels)\n",
    "\n",
    "        print('Training finished')\n",
    "        self.test(inputs, labels)\n",
    "\n",
    "    def test(self, inputs, labels):\n",
    "        \"\"\" The testing routine that run forward pass and report the accuracy.\n",
    "\n",
    "        Args:\n",
    "            inputs: the testing data. One or several data samples are both okay.\n",
    "                The shape is expected to be [BatchSize, 2].\n",
    "            labels: the ground truth correspond to the inputs.\n",
    "        \"\"\"\n",
    "        n = inputs.shape[0]\n",
    "\n",
    "        error = 0.0\n",
    "        for idx in range(n):\n",
    "            result = self.forward(inputs[idx:idx+1, :])\n",
    "            error += abs(result - labels[idx:idx+1, :])\n",
    "\n",
    "        error /= n\n",
    "\n",
    "\n",
    "        \"\"\" Print or plot your results in your preferred forms\"\"\"\n",
    "        print('accuracy: %.2f' % ((1 - error)*100) + '%')\n",
    "        \n",
    "        \"\"\" FILL IN HERE \"\"\"\n",
    "        # print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jK3VNavIFpa8"
   },
   "source": [
    "### Run \"Linear\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "id": "urzfLhyhET7b",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (1,100) (1,10) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_288519/530183649.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSimpleNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mpred_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_288519/11412669.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, inputs, labels)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;34m\"\"\" apply your backward function: \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m                 \u001b[0;34m\"\"\" FILL IN HERE \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_interval\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_288519/11412669.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;31m# tmp = der_sigmoid(dot(gradient, self.w2.transpose()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw2\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mder_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml1_out\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m         \u001b[0;31m# tmp = der_sigmoid(dot(gradient, self.w1.transpose()))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw1\u001b[0m \u001b[0;34m-=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (1,100) (1,10) "
     ]
    }
   ],
   "source": [
    "\"\"\" Customize your own code if needed \"\"\"\n",
    "\n",
    "data, label = GenData.fetch_data('Linear', 100)\n",
    "\n",
    "net = SimpleNet(100, 10)\n",
    "net.train(data, label)\n",
    "\n",
    "pred_result = np.round(net.forward(data))\n",
    "SimpleNet.plot_result(data, label, pred_result)\n",
    "\n",
    "\"\"\" FILL IN HERE \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJHxIo2yFeG8"
   },
   "source": [
    "### Run \"XOR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QzEKq9L5Ffu3"
   },
   "outputs": [],
   "source": [
    "\"\"\" Customize your own code if needed \"\"\"\n",
    "\n",
    "data, label = GenData.fetch_data('XOR', 100)\n",
    "\n",
    "net = SimpleNet(100, num_step=100)\n",
    "net.train(data, label)\n",
    "\n",
    "pred_result = np.round(net.forward(data))\n",
    "SimpleNet.plot_result(data, label, pred_result.T)\n",
    "\n",
    "\"\"\" FILL IN HERE \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, label = GenData.fetch_data('Linear', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.859273060080913e-01\n",
      "[[0.6184718  0.75624752 0.62158052 0.71513912 0.61094739 0.68402495\n",
      "  0.53224593 0.6406115 ]]\n",
      "[[0.90704935 0.90688758 0.92456266 0.93066555 0.94664702 0.92553423\n",
      "  0.93317997 0.90788347]]\n",
      "[[0.98147076]]\n",
      "(1, 2)\n"
     ]
    }
   ],
   "source": [
    "print(data[0][1])\n",
    "d = data[0:1, :]\n",
    "w1 = np.random.uniform(0, 1, (d.shape[1], 8))\n",
    "w2 = np.random.uniform(0, 1, (w1.shape[1], 8))\n",
    "w3 = np.random.uniform(0, 1, (w2.shape[1], 1))\n",
    "\n",
    "print(sigmoid(dot(d, w1)))\n",
    "print(sigmoid(dot(sigmoid(dot(d, w1)), w2)))\n",
    "print(sigmoid(dot(sigmoid(dot(sigmoid(dot(d, w1)), w2)), w3)))\n",
    "print(d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9045116114239506e-01\n"
     ]
    }
   ],
   "source": [
    "print(sigmoid(-0.0382))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = SimpleNet(100, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-3.75"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a = net.forward(data[0:2, :])\n",
    "# print(a, '\\n')\n",
    "# print(net.w1, '\\n')\n",
    "# err = a - label[0:2, :]\n",
    "# print(err, '\\n')\n",
    "# net.backward(err)\n",
    "# print(net.w1)\n",
    "# net.test(data[15:50, :], label[15:50, :])\n",
    "# b = net.forward(data[15:50, :])\n",
    "# print(b)\n",
    "# print(label[15:50, :])\n",
    "# net.train(data, label)\n",
    "# print(net.w1)\n",
    "der_sigmoid(2.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.99743368]\n",
      " [0.99743368]]\n"
     ]
    }
   ],
   "source": [
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs 0: \n",
      "accuracy: 0.00%\n",
      "Epochs 10: \n",
      "accuracy: 0.00%\n",
      "Epochs 20: \n",
      "accuracy: 0.00%\n",
      "Epochs 30: \n",
      "accuracy: 0.00%\n",
      "Epochs 40: \n",
      "accuracy: 0.00%\n",
      "Epochs 50: \n",
      "accuracy: 0.00%\n",
      "Epochs 60: \n",
      "accuracy: 0.00%\n",
      "Epochs 70: \n",
      "accuracy: 0.00%\n",
      "Epochs 80: \n",
      "accuracy: 0.00%\n",
      "Epochs 90: \n",
      "accuracy: 0.00%\n",
      "Training finished\n",
      "accuracy: 0.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_288519/167886730.py:12: RuntimeWarning: overflow encountered in multiply\n",
      "  return y * (1 - y)\n"
     ]
    }
   ],
   "source": [
    "net = SimpleNet(100, 10)\n",
    "net.train(data[0:2, :], label[0:2, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]\n",
      " [6]]\n"
     ]
    }
   ],
   "source": [
    "a = np.asarray([[1],\n",
    "                [2]])\n",
    "b = np.asarray([[2],\n",
    "                [3]])\n",
    "print(a*b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]]\n"
     ]
    }
   ],
   "source": [
    "a = np.array([[1]])\n",
    "b = np.array([[2],\n",
    "              [3]])\n",
    "print(b-a)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Lab_1_sample.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
